# -*- coding: utf-8 -*-
import os, re, time, html
from datetime import datetime, timezone
from urllib.parse import urlencode

import feedparser
import requests
from dateutil import parser as dtparser

UTC = timezone.utc

# ===== ä½ å¯ä»¥åœ¨è¿™é‡Œå¢å‡ RSS æº =====
FEEDS = [
    # --- ç ”ç©¶è®ºæ–‡ ---
    "http://export.arxiv.org/rss/cs.AI",   # arXiv: Artificial Intelligenceï¼ˆå®˜æ–¹RSSï¼‰
    "http://export.arxiv.org/rss/cs.RO",   # arXiv: Roboticsï¼ˆå®˜æ–¹RSSï¼‰

    # --- å®˜æ–¹åšå®¢ / å‚å•† ---
    "https://blog.google/technology/google-deepmind/rss/",  # Google DeepMindï¼ˆé¡µé¢æä¾›RSSå…¥å£ï¼‰
    "https://blog.google/technology/ai/rss/",               # Google AIï¼ˆæŠ€æœ¯æ ç›®RSSï¼‰
    "https://ai.meta.com/blog/rss.xml",                     # Meta AIï¼ˆè‹¥å¤±æ•ˆå¯ç”¨ RSSHub æˆ–æ”¹ä¸ºç«™ç‚¹æŠ“å–ï¼‰
    "https://openai.com/blog/rss.xml",                      # OpenAI Blogï¼ˆæœ‰ç¤¾åŒºç¡®è®¤RSSï¼‰
    "https://nvidianews.nvidia.com/rss",                    # NVIDIA æ–°é—»ç¨¿RSS
    # ä¹Ÿå¯ï¼šNVIDIA æŠ€æœ¯åšå®¢åˆ†ç±» RSSï¼ˆè‹¥å¯ç”¨ï¼‰: https://blogs.nvidia.com/blog/category/robotics/feed/

    # --- è¡Œä¸šåª’ä½“ / æœºå™¨äºº ---
    "https://spectrum.ieee.org/rss/robotics/fulltext",      # IEEE Spectrum Roboticsï¼ˆå…¨æ–‡RSSï¼‰
    "https://www.therobotreport.com/feed",                  # The Robot Report
]

# å…³é”®è¯ç”¨äº GitHub Trending è¿‡æ»¤
KEYWORDS = [
    r"\bAI\b", r"\bLLM\b", r"robot", r"robotic", r"reinforcement learning",
    r"machine learning", r"deep learning", r"foundation model", r"autonomous",
    r"ROS\b", r"SLAM\b", r"sim2real", r"manipulation", r"quadruped"
]
KEY_RE = re.compile("|".join(KEYWORDS), re.I)

# GitHub APIï¼ˆå¯é€‰ï¼‰ï¼šç”¨äºè·å–ä»“åº“ topics æ›´ç²¾å‡†è¿‡æ»¤
GH_TOKEN = os.getenv("GH_TOKEN", "").strip()

def normalize_dt(d):
    if not d:
        return None
    try:
        return dtparser.parse(d).astimezone(UTC)
    except Exception:
        return None

def fetch_feed(url, limit=40):
    try:
        data = feedparser.parse(url)
        items = []
        for e in data.entries[:limit]:
            title = html.unescape(getattr(e, "title", "") or "").strip()
            link  = getattr(e, "link", "") or ""
            summary = html.unescape(getattr(e, "summary", "") or "")\
                      .replace("\n", " ").strip()
            published = normalize_dt(getattr(e, "published", None) or getattr(e, "updated", None))
            items.append({
                "source": url,
                "title": title,
                "link": link,
                "summary": summary,
                "published": published or datetime.now(UTC),
            })
        return items
    except Exception as ex:
        return []

def gh_get(url):
    headers = {"Accept": "application/vnd.github+json",
               "User-Agent": "ai-robotics-daily"}
    if GH_TOKEN:
        headers["Authorization"] = f"Bearer {GH_TOKEN}"
    r = requests.get(url, headers=headers, timeout=20)
    r.raise_for_status()
    return r.json()

def fetch_trending_filtered(since="daily", limit=50, want=20):
    """
    æŠ“ GitHub Trendingï¼ˆå…¨ç«™ï¼‰ï¼Œå†ç”¨å…³é”®è¯/Topic è¿‡æ»¤åˆ° AI/æœºå™¨äººç›¸å…³
    """
    base = "https://github.com/trending"
    r = requests.get(base, params={"since": since}, timeout=30,
                     headers={"User-Agent":"Mozilla/5.0"})
    r.raise_for_status()
    html_txt = r.text

    # ç®€å•è§£æ trending åˆ—è¡¨
    rows = re.findall(r'<article[^>]*class="Box-row"[^>]*>(.*?)</article>', html_txt, flags=re.S)
    repos = []
    for row in rows[:limit]:
        m = re.search(r'href="/([^/]+/[^"]+)"', row)
        if not m: 
            continue
        full = m.group(1)  # owner/name
        href = f"https://github.com/{full}"
        desc = ""
        md = re.search(r'<p[^>]*>(.*?)</p>', row, flags=re.S)
        if md:
            desc = re.sub("<.*?>", "", md.group(1)).strip()
        repos.append({"full": full, "url": href, "desc": desc})

    results = []
    for rp in repos:
        hit = bool(KEY_RE.search(rp["full"])) or bool(KEY_RE.search(rp["desc"]))
        # å¦‚æœæœ‰ GH_TOKENï¼Œå†ç”¨ topics è¿‡æ»¤ä¸€æ¬¡
        if GH_TOKEN:
            try:
                topics = gh_get(f"https://api.github.com/repos/{rp['full']}/topics").get("names", [])
                if any(KEY_RE.search(t) for t in topics):
                    hit = True
            except Exception:
                pass
        if hit:
            results.append(rp)
        if len(results) >= want:
            break
    return results

def build_markdown():
    now = datetime.now(UTC)
    title = f"# AI & æœºå™¨äºº æŠ€æœ¯çƒ­ç‚¹æ—¥æŠ¥ï¼ˆ{now.strftime('%Y-%m-%d %H:%M UTC')}ï¼‰\n"
    intro = (
        "\n> æ¥æºåŒ…å«ï¼šarXivï¼ˆcs.AI/cs.ROï¼‰ã€OpenAI/DeepMind/Google/Meta/NVIDIAã€IEEE Spectrum Roboticsã€"
        "The Robot Report ç­‰å®˜æ–¹/åª’ä½“ RSSï¼Œä»¥åŠæŒ‰å…³é”®è¯/Topic è¿‡æ»¤çš„ GitHub Trendingã€‚\n\n"
    )

    # 1) èšåˆ RSS
    feed_items = []
    for u in FEEDS:
        feed_items.extend(fetch_feed(u, limit=40))
        time.sleep(0.5)  # è½»å¾®é™é€Ÿ

    # å»é‡ & æŒ‰æ—¶é—´æ’åº
    seen = set()
    uniq = []
    for it in feed_items:
        key = (it["title"][:120], it["link"])
        if key in seen:
            continue
        seen.add(key)
        uniq.append(it)
    uniq.sort(key=lambda x: x["published"] or datetime(1970,1,1, tzinfo=UTC), reverse=True)

    # 2) GitHub Trending è¿‡æ»¤
    gh_daily  = fetch_trending_filtered("daily", want=20)
    gh_weekly = fetch_trending_filtered("weekly", want=20)

    def section_rss(maxn=40):
        md = ["## ğŸ“¡ æœ€æ–°èµ„è®¯ï¼ˆRSS èšåˆï¼‰\n",
              "| æ—¶é—´(UTC) | æ ‡é¢˜ | æ¥æº |\n",
              "|---|---|---|\n"]
        for it in uniq[:maxn]:
            src = it["source"].split("/")[2]
            ts = (it["published"] or now).strftime("%Y-%m-%d %H:%M")
            title = it["title"].replace("|", "/")
            md.append(f"| {ts} | [{title}]({it['link']}) | {src} |")
        md.append("\n")
        return "\n".join(md)

    def section_trending(label, items):
        md = [f"## â­ GitHub Trendingï¼ˆ{label}ï¼ŒAI/æœºå™¨äººè¿‡æ»¤ï¼‰\n",
              "| # | ä»“åº“ | ç®€ä»‹ |\n",
              "|---:|---|---|\n"]
        for i, it in enumerate(items, 1):
            name = it["full"]
            desc = (it["desc"] or "-").replace("|","/").strip()
            md.append(f"| {i} | [{name}]({it['url']}) | {desc} |")
        md.append("\n")
        return "\n".join(md)

    parts = [
        title, intro,
        section_rss(60),
        section_trending("Daily", gh_daily),
        section_trending("Weekly", gh_weekly),
        "_è‡ªåŠ¨ç”Ÿæˆ Â· é…ç½®ä¸è„šæœ¬è§ `ai_robotics_daily.py`ã€‚_\n"
    ]
    return "\n".join(parts)

def main():
    md = build_markdown()
    with open("AI_ROBOTICS_DAILY.md", "w", encoding="utf-8") as f:
        f.write(md)

if __name__ == "__main__":
    main()
